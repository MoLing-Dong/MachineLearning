K近邻算法（**K-Nearest Neighbors，简称 KNN**）是一种**基本的、经典的监督学习算法**，用于**分类**和**回归**
任务，尤其以分类问题最为常见。它的核心思想是：

> **给定一个测试样本，根据距离它最近的 K 个训练样本的类别，来决定它的预测类别。**

---

### 📌 一句话理解

> **KNN 是“看邻居怎么选，我就怎么选”** —— 一个点所属的类别由它周围 K 个最近点的多数投票决定。

---

## 一、KNN 的核心原理

KNN 不需要建模训练过程，而是在**预测阶段**使用以下步骤：

### 步骤说明：

1. **确定参数 K**（即“邻居”的个数）。
2. **计算距离**：对测试点，计算其与训练集中所有点之间的距离（常用欧几里得距离）。
3. **选择最近的 K 个点**：从训练集中选出距离测试点最近的 K 个样本。
4. **投票/加权**：

    * **分类问题**：这 K 个样本中，出现次数最多的类别作为预测结果（多数投票）。
    * **回归问题**：对这 K 个样本的标签值求平均或加权平均，作为预测结果。

---

## 二、KNN 图解（示意）

```
●：类别A     ■：类别B     ☆：待分类点

类别A：
●   ●   ●
       ☆
类别B：
    ■   ■   ■

K=3，最近的3个邻居是：[●, ■, ■] → 类别B票数最多 → ☆ 归为 B 类
```

---

## 三、KNN 的距离度量（常用）

* **欧几里得距离（L2）**：

  $$
  d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
  $$

* **曼哈顿距离（L1）**：

  $$
  d(x, y) = \sum_{i=1}^n |x_i - y_i|
  $$

* **余弦相似度**（主要用于文本数据）等。

---

## 四、KNN 的 Python 实现（分类问题）

我们用 sklearn 来实现 KNN 进行分类：

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 加载数据
iris = load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建 KNN 模型（K=3）
knn = KNeighborsClassifier(n_neighbors=3)

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 输出准确率
print(f"KNN 准确率: {accuracy_score(y_test, y_pred):.2f}")
```

---

## 五、K 值的选择

K 的选择非常关键：

* K 太小（如 K=1）：

    * 容易受噪声影响，模型**高方差、易过拟合**。
* K 太大：

    * 模型**高偏差，欠拟合**，可能将远离测试点的样本也考虑进来。

通常通过**交叉验证**或经验调参选择最优 K 值。

---

## 六、KNN 的优缺点

| 优点           | 缺点                |
|--------------|-------------------|
| 简单直观，无需训练过程  | 计算开销大（对所有训练点计算距离） |
| 适用于小样本、多分类问题 | 对特征缩放敏感（需归一化）     |
| 理论成熟，效果好，易解释 | 维度灾难：高维空间距离计算失效   |

---

## 七、KNN 使用建议

* **适用于样本数不太大、特征维度适中**的问题。
* 需要对特征进行**归一化或标准化**（否则某一维度尺度大将主导距离计算）。
* 可与 KD-Tree、Ball Tree 等结构结合提升搜索效率。

---

## 八、KNN 与其他算法的比较

| 算法   | 是否训练 | 可解释性 | 训练速度   | 预测速度 |
|------|------|------|--------|------|
| KNN  | ❌    | ✅    | ✅（无训练） | ❌（慢） |
| 决策树  | ✅    | ✅    | ✅      | ✅    |
| 逻辑回归 | ✅    | ✅    | ✅      | ✅    |
| 神经网络 | ✅    | ❌    | ❌      | ✅    |

---

## 总结

KNN 是一种**基于实例的、延迟学习（lazy learning）**的算法，核心思想是**看邻居的多数投票**。虽然 KNN 实现简单，但在实际应用中往往需要注意：

* 数据标准化
* 特征选择
* 合理选择 K 值
* 对于大数据量，优化计算结构

---

如果你希望我手写实现一个简单的 KNN，或了解它的回归版本，也可以告诉我。需要现在写一版手工实现代码吗？
