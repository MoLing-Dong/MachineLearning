**决策树回归（Decision Tree Regression）** 是一种非线性监督学习算法，用于解决**回归问题**（即预测连续值）。它的基本思想与分类树类似，只是叶子节点的输出不再是类别标签，而是一个**实数值**（例如房价、温度等）。

---

## 一、核心思想

通过**划分特征空间**，把数据集分割成多个区域，每个区域用一个固定的值（如均值）来预测输出目标值。

> 例如：若特征是「面积」，可将其划分为 <50㎡, 50-100㎡, >100㎡ 三段，每段分别预测不同的平均房价。

---

## 二、构建流程（简略）

1. **选择最优特征与划分点**（使得左右子树预测误差最小）；
2. **递归分裂**数据集，构建子树；
3. **停止条件**：

   * 到达最大深度；
   * 样本数小于最小阈值；
   * 当前节点样本的输出值方差过小（近似一致）。

---

## 三、损失函数：均方误差（MSE）

在每个节点划分时，选择使得划分后**加权均方误差最小**的特征和切分点。

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^2
$$

在划分时目标是：

$$
\min_{j,s} \left[ \frac{n_L}{n} \cdot MSE_L + \frac{n_R}{n} \cdot MSE_R \right]
$$

其中 $j$ 是划分特征，$s$ 是切分值。

---

## 四、Python 示例（用 scikit-learn 实现）

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# 构造非线性数据
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + 0.1 * np.random.randn(80)

# 建模
regressor = DecisionTreeRegressor(max_depth=3)
regressor.fit(X, y)

# 预测与可视化
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_pred = regressor.predict(X_test)

plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="data")
plt.plot(X_test, y_pred, color="cornflowerblue", label="prediction")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Decision Tree Regression")
plt.legend()
plt.show()
```

---

## 五、优缺点

### ✅ 优点：

* 非线性建模能力强；
* 可处理高维数据；
* 对特征归一化不敏感；
* 模型可解释性强（结构清晰、可视化）；

### ❌ 缺点：

* 容易**过拟合**；
* 对数据的小变动敏感（可通过剪枝或集成学习缓解）；
* 模型边界较硬（预测曲线不平滑）；

---

## 六、应用场景

* 房价预测；
* 产品销量预测；
* 股票波动预测；
* 医疗数值指标预估；
* 任何需要连续数值预测的场景。

---

## 七、延伸：与分类树的区别

| 特性     | 分类树（Classification Tree） | 回归树（Regression Tree） |
| ------ | ------------------------ | -------------------- |
| 目标变量类型 | 离散类别                     | 连续数值                 |
| 叶节点输出  | 类别标签                     | 连续预测值（均值）            |
| 损失函数   | Gini/熵                   | 均方误差（MSE）            |

---

如果你希望我手动实现一个回归树的核心流程（如贪心分裂+终止条件），我也可以为你写一个简化版代码。是否还想了解回归树与随机森林回归的区别？
