### 逻辑回归（Logistic Regression）简介

**逻辑回归**是一种用于分类问题的线性模型。尽管名字中有“回归”二字，但它通常用于解决**二分类问题**。其核心思想是将线性回归的输出通过一个**逻辑函数（Logistic Function）**（也称为Sigmoid函数）映射到 \( [0, 1] \) 区间，表示概率。

### 逻辑回归模型

假设有输入特征 \( X \)，目标输出 \( y \)，我们希望通过训练来找出一个最佳的决策边界。

1. **线性模型**：
$$
z = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_n \cdot x_n
$$
其中 \( w_0 \) 为偏置项，\( w_1, w_2, \dots, w_n \) 为特征的权重。

2. **Sigmoid函数**：
逻辑回归通过对线性组合结果应用 Sigmoid 函数，将其转化为概率：
$$
h_\theta(x) = \frac{1}{1 + e^{-z}}
$$
其中 \( z \) 是线性组合结果，\( h_\theta(x) \) 是概率值。

3. **决策边界**：
根据输出概率 \( h_\theta(x) \)，我们可以设定一个阈值（通常是0.5）。如果 \( h_\theta(x) \geq 0.5 \)，预测为类别1（正类），否则为类别0（负类）。

### 目标函数：交叉熵损失

为了找到最佳参数 \( w_0, w_1, \dots, w_n \)，我们使用**交叉熵损失**函数（Log Loss）来衡量模型预测值与真实标签之间的差异。其损失函数定义为：
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \cdot \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \cdot \log(1 - h_\theta(x^{(i)})) \right)
$$
其中，\( m \) 是样本数量，\( y^{(i)} \) 是第 \( i \) 个样本的真实标签。

### 逻辑回归的简单实现（使用Python和NumPy）

我们来实现一个简单的逻辑回归模型，并通过梯度下降法优化参数。

```python
import numpy as np
import matplotlib.pyplot as plt

# 生成数据：两类数据，类别0和类别1
np.random.seed(42)
m = 100  # 样本数量
X = np.random.randn(m, 2)  # 生成100个二维数据点
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # 如果 x1 + x2 > 0，则为1，否则为0

# 绘制数据分布
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Generated Data')
plt.show()

# 逻辑回归的Sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 损失函数（交叉熵损失）
def compute_cost(X, y, theta):
    m = len(y)
    z = X.dot(theta)
    h = sigmoid(z)
    cost = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

# 梯度下降
def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    cost_history = []

    for i in range(iterations):
        # 计算预测
        z = X.dot(theta)
        h = sigmoid(z)
        
        # 计算梯度
        gradient = (1/m) * X.T.dot(h - y)
        
        # 更新theta
        theta -= learning_rate * gradient
        
        # 计算当前的损失
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)
    
    return theta, cost_history

# 数据预处理：添加偏置项（全为1的列）
X_b = np.c_[np.ones((m, 1)), X]  # 在X中添加偏置项
theta_initial = np.zeros(X_b.shape[1])  # 初始化theta为0

# 设置学习率和迭代次数
learning_rate = 0.1
iterations = 1000

# 使用梯度下降法训练模型
theta_optimal, cost_history = gradient_descent(X_b, y, theta_initial, learning_rate, iterations)

# 打印最终的theta值
print("Optimal theta:", theta_optimal)

# 绘制损失函数的变化过程
plt.plot(range(iterations), cost_history, color='blue')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Cost function during training')
plt.show()

# 预测函数
def predict(X, theta):
    z = X.dot(theta)
    return sigmoid(z) >= 0.5

# 使用训练好的模型进行预测
y_pred = predict(X_b, theta_optimal)

# 计算准确率
accuracy = np.mean(y_pred == y) * 100
print(f'Accuracy: {accuracy:.2f}%')

# 绘制决策边界
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
Z = predict(np.c_[np.ones((xx.ravel().shape[0], 1)), xx.ravel(), yy.ravel()], theta_optimal)
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.Paired)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Boundary')
plt.show()
```

### 代码解释

1. **数据生成与可视化**：
   - 生成两个特征的100个数据点，类别为0或1，标签由 \( x_1 + x_2 \) 是否大于0决定。然后使用 `matplotlib` 绘制数据点，查看其分布情况。

2. **Sigmoid函数**：
   - 用于将线性组合结果映射到 \( [0, 1] \) 之间，输出为预测的概率。

3. **损失函数**：
   - 使用**交叉熵损失**（Log Loss）来度量模型的预测误差。

4. **梯度下降**：
   - 通过梯度下降法来最小化损失函数，迭代更新模型参数 \(theta\)。

5. **预测与决策边界**：
   - 根据训练得到的模型参数 \(theta\)，我们对数据进行预测，并绘制决策边界，显示模型如何区分不同类别。

6. **结果展示**：
   - 输出最终的模型参数和训练过程中损失函数的变化。
   - 最后，绘制决策边界，展示模型如何对数据进行分类。

### 输出
- 最终输出会显示训练得到的最优参数 \(theta\)。
- 绘制损失函数随迭代次数变化的图表，帮助我们了解模型训练过程。
- 计算并显示分类的准确率。
- 绘制决策边界，显示模型如何分隔两类数据。

### 总结
这是一个简单的**逻辑回归**实现，使用了**梯度下降法**来优化模型参数。这个实现适用于二分类问题，且通过训练过程和损失函数的可视化可以帮助理解模型的工作原理。你可以根据自己的数据集和需求，调整学习率和迭代次数等超参数。